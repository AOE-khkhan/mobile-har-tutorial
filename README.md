
# MobileHCI 2018 tutorial: Machine Learning for Intelligent Mobile User Interfaces using Keras
### Part II: Human Activity Recognition

This Human Activity Recognition (HAR) tutorial is part of the MobileHCI 2018 tutorial on using machine learning for building intelligent mobile user interfaces using the Keras deep learning library.

In this part, we will mainly cover:
* [`01_data-preprocessing.ipynb`](https://github.com/cwi-dis/MobileHCI2018-HAR-Tutorial/blob/master/01_data-preprocessing.ipynb): how to preprocess and explore an existing HAR dataset that uses wearable IMU sensors. We will use the USC-HAD dataset [(Zhang et al., 2012)](https://dl.acm.org/citation.cfm?id=2370438)
* [`02_model-training.ipynb`](https://github.com/cwi-dis/MobileHCI2018-HAR-Tutorial/blob/master/02_model-training.ipynb): how to train and evaluate a convolutional LSTM neural network model to predict 12 daily activities
* [`03_model-export.ipynb`](https://github.com/cwi-dis/MobileHCI2018-HAR-Tutorial/blob/master/03_model-export.ipynb): provided there is time, we will show how to export the trained model for us in an Android HAR app

This tutorial will be held at:
* [MobileHCI'18, 3rd September 2018, 09:00-17:00, Barcelona, Spain](https://mobilehci.acm.org/2018/2018/06/23/tutorials/#tut1) - [Tutorial Website](https://interactionlab.io/blog/2018/06/18/imui-mobilehci18/)
