# coding: utf-8

# importing libraries and dependecies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, LSTM, LSTMCell, Bidirectional, TimeDistributed, InputLayer, ConvLSTM2D

#from keras import backend as K
from keras import optimizers
#K.set_image_dim_ordering('th')

# import numpy as np
# import matplotlib
# import matplotlib.pyplot as plt
# import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)
# import pandas as pd
import os
import scipy.io
import pickle
import math
from mpl_toolkits.mplot3d import Axes3D
from scipy import stats
import tensorflow as tf
import seaborn as sns
import pylab
from sklearn import metrics
#from tensorflow.python.client import device_lib
from sklearn.model_selection import train_test_split, cross_val_score


#print(device_lib.list_local_devices())

# setting up a random seed for reproducibility
random_seed = 611
np.random.seed(random_seed)


#with tf.device('/gpu:0'):
 #   a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
 #   b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
 #   c = tf.matmul(a, b)

#with tf.Session() as sess:
#    print (sess.run(c))


# matplotlib inline
plt.style.use('ggplot')
# defining function for loading the dataset
def readData(filePath):
    # attributes of the dataset
    columnNames = ['user_id','activity','timestamp','x-axis','y-axis','z-axis']
    data = pd.read_csv(filePath,header = None, names=columnNames,na_values=';')
    return data[0:2000]
# defining a function for feature normalization
# (feature - mean)/stdiv
def featureNormalize(dataset):
    mu = np.mean(dataset,axis=0)
    sigma = np.std(dataset,axis=0)
    return (dataset-mu)/sigma
# defining the function to plot a single axis data
def plotAxis(axis,x,y,title):
    axis.plot(x,y)
    axis.set_title(title)
    axis.xaxis.set_visible(False)
    axis.set_ylim([min(y)-np.std(y),max(y)+np.std(y)])
    axis.set_xlim([min(x),max(x)])
    axis.grid(True)
# defining a function to plot the data for a given activity
def plotActivity(activity,data):
    fig,(ax0,ax1,ax2) = plt.subplots(nrows=3, figsize=(15,10),sharex=True)
    plotAxis(ax0,data['timestamp'],data['x-axis'],'x-axis')
    plotAxis(ax1,data['timestamp'],data['y-axis'],'y-axis')
    plotAxis(ax2,data['timestamp'],data['z-axis'],'z-axis')
    plt.subplots_adjust(hspace=0.2)
    fig.suptitle(activity)
    plt.subplots_adjust(top=0.9)
    plt.show()
# defining a window function for segmentation purposes
def windows(data,size):
    start = 0
    while start< data.count():
        yield int(start), int(start + size)
        start+= (size/2)

def segment_signal_had(data, window_size = 90):
    segments = np.empty((0,window_size,6))
    labels= np.empty((0))
#     print labels
    for (start, end) in windows(data['activity'],window_size):
        x = data['acc_x'][start:end]
        y = data['acc_y'][start:end]
        z = data['acc_z'][start:end]
        p = data['gyr_x'][start:end]
        q = data['gyr_y'][start:end]
        r = data['gyr_z'][start:end]

        if(len(data['activity'][start:end])==window_size):
            segments = np.vstack([segments,np.dstack([x,y,z,p,q,r])])
            labels = np.append(labels,stats.mode(data['activity'][start:end])[0][0])
    return segments, labels



## read in the USC-HAD data
DIR = './data/USC-HAD/data/'

# activity = []
subject = []
# age = []
act_num = []
sensor_readings = []

def read_dir(directory):
    for path, subdirs, files in os.walk(DIR):
        for name in files:
            if name.endswith('.mat'):
                mat = scipy.io.loadmat(os.path.join(path, name))
#                 activity.append(mat['activity'])
                subject.extend(mat['subject'])
#                 age.extend(mat['age'])
                sensor_readings.append(mat['sensor_readings'])

                if mat.get('activity_number') is None:
                    act_num.append('11')
                else:
                    act_num.append(mat['activity_number'])
    return subject, act_num, sensor_readings

## UNCOMMENT to handle corrupt datapoint
# act_num[258] = '11'
subject, act_num, sensor_readings = read_dir(DIR)


# handle corrupt datapoint
#act_num[258] = '11'

#act_label2 = []

#for i in range(len(sensor_readings)-1):

#    act_label2.extend(act_num[i])

#act_label_int = [int(elem) for elem in act_label2]

#print len(act_label2)
#y_ = np.reshape(np.array(act_label_int), (np.array(act_label_int).shape[0],1))


## Get acc + gyr sensor readings and put in df (dataframe)
acc_x = []
acc_y = []
acc_z = []
gyr_x = []
gyr_y = []
gyr_z = []

act_label = []
subject_id = []
df = None

for i in range(840):
    for j in sensor_readings[i]:

        acc_x.append(j[0]) # acc_x
        acc_y.append(j[1]) # acc_y
        acc_z.append(j[2]) # acc_z
        gyr_x.append(j[3]) # gyr_x
        gyr_y.append(j[4]) # gyr_y
        gyr_z.append(j[5]) # gyr_z
        act_label.append(act_num[i])
        subject_id.append(subject[i])

df = pd.DataFrame({'subject':subject_id,'acc_x':acc_x,'acc_y':acc_y,'acc_z':acc_z,'gyr_x':gyr_x,'gyr_y':gyr_y,'gyr_z':gyr_z,'activity':act_label})


df = df[['subject','acc_x', 'acc_y', 'acc_z', 'gyr_x', 'gyr_y', 'gyr_z','activity']]

df.loc[df['activity'] == '1', 'activity'] = 'Walking Forward'
df.loc[df['activity'] == '2', 'activity'] = 'Walking Left'
df.loc[df['activity'] == '3', 'activity'] = 'Walking Right'
df.loc[df['activity'] == '4', 'activity'] = 'Walking Upstairs'
df.loc[df['activity'] == '5', 'activity'] = 'Walking Downstairs'
df.loc[df['activity'] == '6', 'activity'] = 'Running Forward'
df.loc[df['activity'] == '7', 'activity'] = 'Jumping Up'
df.loc[df['activity'] == '8', 'activity'] = 'Sitting'
df.loc[df['activity'] == '9', 'activity'] = 'Standing'
df.loc[df['activity'] == '10', 'activity'] = 'Sleeping'
df.loc[df['activity'] == '11', 'activity'] = 'Elevator Up'
df.loc[df['activity'] == '12', 'activity'] = 'Elevator Down'

## These are the 12 classes we want to recognize!
df['activity'].unique()

## print size of dataset
print 'df size ' + str(len(df))

## Leave one out (remove subject 1) for later testing

df_1_out = df[df['subject'] == '1']
df = df[df['subject'] != '1']

# segmenting the signal in overlapping windows of 90 samples with 50% overlap
segments, labels = segment_signal_had(df)

segments_test, labels_test = segment_signal_had(df_1_out)

# # open a file, where you stored the pickled data
# segments = open('segments_90.p','rb')
# labels = open('labels_90.p', 'rb')
# # dump information to that file
# segments = pickle.load(segments)
# labels = pickle.load(labels)
pickle.dump(segments, open( "segments_90_1out.p","wb"))
pickle.dump(labels, open( "labels_90_1out.p","wb"))

pickle.dump(segments_test, open( "segments_90_1out_test.p","wb"))
pickle.dump(labels_test, open( "labels_90_1out_test.p","wb"))

#categorically defining the classes of the activities
labels = np.asarray(pd.get_dummies(labels),dtype = np.int8)
labels_test = np.asarray(pd.get_dummies(labels_test),dtype = np.int8)

#labels = np.delete(labels, 2, 1)

# defining parameters for the input and network layers
# we are treating each segmeent or chunk as a 2D image (90 X 3)
numOfRows = segments.shape[1]
numOfColumns = segments.shape[2]

numOfRows_test = segments_test.shape[1]
numOfColumns_test = segments_test.shape[2]

numChannels = 1
numFilters = 128 # number of filters in Conv2D layer
# kernal size of the Conv2D layer
kernalSize1 = 2
# max pooling window size
poolingWindowSz = 2
# number of filters in fully connected layers
numNueronsFCL1 = 128
numNueronsFCL2 = 128
# split ratio for test and validation
trainSplitRatio = 0.8
# number of epochs
Epochs = 20
# batchsize
batchSize = 10
# number of total clases
numClasses = labels.shape[1]
# dropout ratio for dropout layer
dropOutRatio = 0.2
# reshaping the data for network input
reshapedSegments = segments.reshape(segments.shape[0], numOfRows, numOfColumns,1)

reshapedSegments_test = segments_test.reshape(segments_test.shape[0], numOfRows_test, numOfColumns_test,1)

# splitting in training and testing data
# trainSplit = np.random.rand(len(reshapedSegments)) < trainSplitRatio
trainX = reshapedSegments
testX = reshapedSegments_test
trainX = np.nan_to_num(trainX)
testX = np.nan_to_num(testX)
trainY = labels
testY = labels_test



print "segments shape:" + str(segments.shape)
print "labels shape:" + str(labels.shape)
print "trainX shape: " + str(trainX.shape)
print "trainY shape: " + str(trainY.shape)
print "testX shape: " + str(testX.shape)
print "testY shape: " + str(testY.shape)

def Conv2D_LSTM_Model():
    model = Sequential()

    # adding the first convolutionial layer with 32 filters and 5 by 5 kernal size, using the rectifier as the activation function
    model.add(ConvLSTM2D(numFilters, (kernalSize1,kernalSize1),input_shape=(None, numOfRows, numOfColumns, 1),activation='relu', padding='same',return_sequences=True))
    print (model.output_shape)

    # adding a maxpooling layer
    model.add(TimeDistributed(MaxPooling2D(pool_size=(poolingWindowSz,poolingWindowSz),padding='valid')))
    print (model.output_shape)

    # adding a dropout layer for the regularization and avoiding over fitting
    model.add(Dropout(dropOutRatio))
    print (model.output_shape)
#     model.add(LSTM(32,input_shape=(numOfRows, numOfColumns, 1),return_sequences=True))

    # flattening the output in order to apply the fully connected layer
    model.add(TimeDistributed(Flatten()))
    print (model.output_shape)
    # adding first fully connected layer with 256 outputs
    model.add(Dense(numNueronsFCL1, activation='relu'))
    print (model.output_shape)

    #adding second fully connected layer 128 outputs
    model.add(Dense(numNueronsFCL2, activation='relu'))
    print (model.output_shape)

    model.add(TimeDistributed(Flatten()))
    print (model.output_shape)

    # adding softmax layer for the classification
    model.add(Dense(numClasses, activation='softmax'))

    # Compiling the model to generate a model
    adam = optimizers.Adam(lr = 0.001, decay=1e-6)
    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
    return model



from sklearn.model_selection import LeaveOneGroupOut

## Train the network!
tf.get_default_graph()

## Leave one group out train-test split
segments, labels, subjects = segment_signal_ucd(df)
groups = np.array(subjects)

logo = LeaveOneGroupOut()
logo.get_n_splits(segments, labels, groups)
logo.get_n_splits(groups=groups)

reshapedSegments = segments.reshape(segments.shape[0], numOfRows, numOfColumns,1)
labels = np.asarray(pd.get_dummies(labels),dtype = np.int8)

cvscores = []

for train_index, test_index in logo.split(reshapedSegments, labels, groups):
    print("TRAIN:", train_index, "TEST:", test_index)
    trainX, testX = reshapedSegments[train_index], reshapedSegments[test_index]
    trainY, testY = labels[train_index], labels[test_index]
#     print(np.nan_to_num(trainX), np.nan_to_num(testX), trainY, testY)

    model = Conv2D_LSTM_Model()
    for layer in model.layers:
        print(layer.name)
    print trainX.shape
    model.fit(np.expand_dims(trainX,1),np.expand_dims(trainY,1), epochs=1,batch_size=batchSize,verbose=2)
    score = model.evaluate(np.expand_dims(testX,1),np.expand_dims(testY,1),verbose=2)
    print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))
    print('Baseline ConvLSTM Error: %.2f%%' %(100-score[1]*100))
    cvscores.append(scores[1] * 100)

print("%.2f%% (+/- %.2f%%)" % (numpy.mean(cvscores), numpy.std(cvscores)))

## Save your model!
model.save('model_hcd_test.h5')
model.save_weights('model_weights_test.h5')
# np.save('groundTruth_test_lstm.npy',np.expand_dims(testY,1))
# np.save('testData_test_lstm.npy',np.expand_dims(testX,1))

## write to JSON, in case you wanrt to work with that data format later when inspecting your model
with open("./data/model_hcd_test.json", "w") as json_file:
  json_file.write(model.to_json())





model = Conv2D_LSTM_Model()

for layer in model.layers:
    print(layer.name)
print trainX.shape
Lprint("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))
print('Baseline ConvLSTM Error: %.2f%%' %(100-score[1]*100))
model.save('model_had_lstm_1out.h5')
model.save_weights('model_weights_had_lstm_1out.h5')
np.save('groundTruth_had_lstm_1out.npy',np.expand_dims(testY,1))
np.save('testData_had_lstm_1out.npy',np.expand_dims(testX,1))
